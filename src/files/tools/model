#!/usr/bin/python3
# -*- coding: UTF-8 -*-
from pbox import *
from tinyscript import *


__author__      = "Alexandre D'Hondt"
__email__       = "alexandre.dhondt@gmail.com"
__version__     = "1.2.6"
__copyright__   = ("A. D'Hondt", 2021)
__license__     = "gpl-3.0"
__doc__         = """
This utility aims to train Machine Learning models based on an input dataset. It currently supports various algorithms
 and allows to select a data scaler. Moreover, it supports Grid Search cross-validation for some algorithms.
"""
__description__ = "Train Machine Learning models"
__examples__    = [
    "list",
    "preprocess my-dataset",
    "train my-dataset -a mnb",
    "show my-dataset",
]


def at_interrupt():
    """ Interrupt handler """
    logger.warn("Interrupted by the user.")


def __add_name(parser, optional=False, force=False, joblib=False):
    def model_exists(string):
        if string == "all":
            return string
        return open_model(string)
    a, kw = ("-n", "--name", ) if optional else ("name", ), {'type': model_exists, 'help': "name of the model"}
    parser.add_argument(*a, **kw)
    return parser


if __name__ == '__main__':
    sparsers = parser.add_subparsers(dest="command", help="command to be executed")
    compare = __add_name(sparsers.add_parser("compare", help="compare the selected model with others"), joblib=True)
    compare.add_argument("-d", "--dataset", type=open_dataset, action="extend", nargs="*",
                         help="dataset to be selected for the comparison")
    compare.add_argument("-i", "--include", action="store_true", help="include unformatted models")
    compare.add_argument("-m", "--model", type=open_model, action="extend", nargs="*",
                         help="model to be added in the comparison")
    __add_name(sparsers.add_parser("edit", help="edit the performance log file"), True)
    listm = sparsers.add_parser("list", help="list all the models from the workspace")
    listm.add_argument("--algorithms", action="store_true", help="show available algorithms instead of models")
    preproc = __add_name(sparsers.add_parser("preprocess", help="preprocess features and visualize the result"))
    preproc.add_argument("-f", "--features-set", type=ts.file_exists, default=config['features'],
                         help="features set's YAML definition")
    __add_name(sparsers.add_parser("purge", help="purge the selected model"))
    rename = __add_name(sparsers.add_parser("rename", help="rename the selected model"))
    rename.add_argument("name2", type=ts.folder_does_not_exist, help="new name of the model")
    __add_name(sparsers.add_parser("show", help="get an overview of the model"))
    test = __add_name(sparsers.add_parser("test", help="test the model on a given input"), joblib=True)
    test.add_argument("executable", help="executable or folder containing executables or dataset or data CSV file")
    test.add_argument("-f", "--features-set", type=ts.file_exists, default=config['features'],
                      help="features set's YAML definition")
    test.add_argument("-u", "--unlabeled", action="store_true", help="do not use assigned labels (if any)")
    test.add_argument("--ignore-labels", action="store_true", help="while computing metrics, only consider those not requiring labels")
    test.add_argument("--sep", default=",", choices=",;|\t", help="set the CSV separator",
                      note="required when using input CSV data instead of a Dataset")
    train = sparsers.add_parser("train", help="train a model on the given dataset")
    train.add_argument("dataset", type=open_dataset, help="dataset for training the model")
    __add_name(train, True)
    train.add_argument("-a", "--algorithm", choices=[a.name for a in Algorithm.registry], default="dt",
                       help="machine learning algorithm to be used\n- %s\n * supports Grid Search cross-validation\n" %\
                            "\n- ".join("%s: %s%s" % (a.name.ljust(8), a.description, ["", "*"][a.parameters.get('cv') \
                            is not None]) for a in Algorithm.registry))
    train.add_argument("--algorithms-set", type=ts.file_exists, default=config['algorithms'],
                       help="algorithms set's YAML definition")
    train.add_argument("-f", "--features-set", type=ts.file_exists, default=config['features'],
                       help="features set's YAML definition")
    train.add_argument("-m", "--multiclass", action="store_true", help="train the model using multiple label classes")
    train.add_argument("-r", "--reset", action="store_true", help="reset the model before (re)training")
    train.add_argument("--cv", default=5, type=ts.pos_int, help="number of Cross-Validation folds")
    train.add_argument("--feature", action="extend", nargs="*", help="list of features to be selected")
    train.add_argument("--ignore-labels", action="store_true", help="while computing metrics, only consider those not requiring labels")
    train.add_argument("--n-jobs", default=N_JOBS, help="number of jobs to be run in parallel")
    train.add_argument("--param", action="extend", nargs="*", type=lambda x: x.split(","),
                       help="comma-separated list of parameters for the algorithm",
                       note="fixing a parameter with this option disables it from the cross-validation")
    visualize = __add_name(sparsers.add_parser("visualize", help="visualize the model"))
    visualize.add_argument("-e", "--export", action="store_true", help="export to PNG")
    visualize.add_argument("-o", "--output-dir", metavar="DIR", default=".", help="output directory")
    viz_dependent = visualize.add_argument_group("options depending on the chosen algorithm", before="extra arguments")
    viz_dependent.add_argument("--imputer-strategy", default="mean", choices=("constant", "mean", "most_frequent"),
                               help="strategy for imputing missing values")
    viz_dependent.add_argument("-n", "--pca-components", metavar="N", default=20, type=int,
                               help="PCA components for dimensionality reduction")
    viz_dependent.add_argument("-p", "--perplexity", metavar="P", default=30, type=int,
                               help="t-SNE perplexity for dimensionality reduction")
    viz_dependent.add_argument("-r", "--reduce-train-data", action="store_true",
                               help="reduce the training data",
                               note="the data is only reduced for the visualization by default")
    viz_dependent.add_argument("-f", "--features", nargs="*",type=lambda x: x.split(","),help="comma separated list of features to be selected", default="")
    initialize(noargs_action="help")
    logger.name = "model"
    logging.configLogger(logger, ["INFO", "DEBUG"][args.verbose], fmt=LOG_FORMATS[args.verbose], relative=True)
    name, args.load = getattr(args, "name", Model(load=False)), args.command not in ["list", "purge"]
    if name == "all":
        if args.command == "purge":
            purged = False
            for m in Model.iteritems(True):
                m.purge()
                purged = True
            if not purged:
                logger.warning("No model to purge in workspace (%s)" % config['models'])
        else:
            logger.warning("Model cannot be named 'all' (reserved word)")
    else:
        if hasattr(args, "algorithms_set"):
            Algorithm.source = args.algorithms_set  # recompute the child classes of Algorithm with a given source
        if hasattr(args, "features_set"):
            Features.source = args.features_set  # configure the features set's YAML definition source
        if args.command == "train":
            name = Model(**vars(args))
        if args.command == "visualize":
            args.viz_params = {}
            for a in ["imputer_strategy", "pca_components", "perplexity", "reduce_train_data", "features"]:
                args.viz_params[a] = getattr(args, a)
                delattr(args, a)
        getattr(name, args.command)(**vars(args))

